{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Document Set:\n",
    "\n",
    "- d1: The sky is blue.\n",
    "- d2: The sun is bright.\n",
    "\n",
    "Test Document Set:\n",
    "\n",
    "- d3: The sun in the sky is bright.\n",
    "- d4: We can see the shining sun, the bright sun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count Vectorizer\n",
      "\n",
      "      blue  bright  sky  sun\n",
      "Doc1     1       0    1    0\n",
      "Doc2     0       1    0    1\n",
      "\n",
      "TD-IDF Vectorizer\n",
      "\n",
      "          blue    bright       sky       sun\n",
      "Doc1  0.707107  0.000000  0.707107  0.000000\n",
      "Doc2  0.000000  0.707107  0.000000  0.707107\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# TfidfVectorizer \n",
    "# CountVectorizer\n",
    "\n",
    "\n",
    "# set of documents\n",
    "train = ['The sky is blue.','The sun is bright.']\n",
    "test = ['The sun in the sky is bright', 'We can see the shining sun, the bright sun.']\n",
    "\n",
    "# instantiate the vectorizer object\n",
    "countvectorizer = CountVectorizer(analyzer= 'word', stop_words='english')\n",
    "tfidfvectorizer = TfidfVectorizer(analyzer='word',stop_words= 'english')\n",
    "\n",
    "# convert th documents into a matrix\n",
    "count_wm = countvectorizer.fit_transform(train)\n",
    "tfidf_wm = tfidfvectorizer.fit_transform(train)\n",
    "\n",
    "#retrieve the terms found in the corpora\n",
    "# if we take same parameters on both Classes(CountVectorizer and TfidfVectorizer) , it will give same output of get_feature_names() methods)\n",
    "#count_tokens = tfidfvectorizer.get_feature_names() # no difference\n",
    "count_tokens = countvectorizer.get_feature_names_out()\n",
    "tfidf_tokens = tfidfvectorizer.get_feature_names_out()\n",
    "\n",
    "df_countvect = pd.DataFrame(data = count_wm.toarray(),index = ['Doc1','Doc2'],columns = count_tokens)\n",
    "df_tfidfvect = pd.DataFrame(data = tfidf_wm.toarray(),index = ['Doc1','Doc2'],columns = tfidf_tokens)\n",
    "\n",
    "\n",
    "print(\"Count Vectorizer\\n\")\n",
    "print(df_countvect)\n",
    "print(\"\\nTD-IDF Vectorizer\\n\")\n",
    "print(df_tfidfvect)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse Matrix form of test data : \n",
      "\n",
      "[[0 1 1 1]\n",
      " [0 1 0 2]]\n"
     ]
    }
   ],
   "source": [
    "#import count vectorize and tfidf vectorise\n",
    "\n",
    "# instantiate the vectorizer object\n",
    "# use analyzer is word and stop_words is english which are responsible for \n",
    "# remove stop words and create word vocabulary\n",
    "countvectorizer = CountVectorizer(analyzer='word' , stop_words='english')\n",
    "terms = countvectorizer.fit_transform(train)\n",
    "term_vectors  = countvectorizer.transform(test)\n",
    "print(\"Sparse Matrix form of test data : \\n\")\n",
    "print(term_vectors.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vector of idf \n",
      "\n",
      "[2.09861229 1.         1.40546511 1.        ]\n",
      "\n",
      "Final tf-idf vectorizer matrix form :\n",
      "\n",
      "[[0.         0.50154891 0.70490949 0.50154891]\n",
      " [0.         0.4472136  0.         0.89442719]]\n"
     ]
    }
   ],
   "source": [
    "# Tranfer  sparse matrix of Countvectorizer to tf-idf by \n",
    "# using TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf = TfidfTransformer(norm='l2')\n",
    "term_vectors.todense()\n",
    "#[0, 1, 1, 1]\n",
    "# [0, 1, 0, 2]\n",
    "tfidf.fit(term_vectors)\n",
    "tf_idf_matrix = tfidf.transform(term_vectors)\n",
    "print(\"\\nVector of idf \\n\")\n",
    "print(tfidf.idf_)\n",
    "print(\"\\nFinal tf-idf vectorizer matrix form :\\n\")\n",
    "print(tf_idf_matrix.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse Matrix form of test data : \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matrix([[0.        , 0.57735027, 0.57735027, 0.57735027],\n",
       "        [0.        , 0.4472136 , 0.        , 0.89442719]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate the vectorizer object\n",
    "# use analyzer is word and stop_words is english which are responsible for \n",
    "# remove stop words and create word vocabulary\n",
    "tfidfvectorizer = TfidfVectorizer(analyzer='word' , stop_words='english',)\n",
    "tfidfvectorizer.fit(train)\n",
    "tfidf_train = tfidfvectorizer.transform(train)\n",
    "tfidf_term_vectors  = tfidfvectorizer.transform(test)\n",
    "print(\"Sparse Matrix form of test data : \\n\")\n",
    "tfidf_term_vectors.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
