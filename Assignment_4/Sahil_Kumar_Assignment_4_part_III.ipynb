{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Natural-Language-Processing-YU/M3_Assignment/blob/main/scripts/m3_assignment_part_III.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QD5ia2HsZpsC"
      },
      "source": [
        "# Part III\n",
        "Using the previous two tutorials, please answer the following using an encorder-decoder approach and an LSTM compared approach.\n",
        "\n",
        "Please create a transformer-based classifier for English name classification into male or female.\n",
        "\n",
        "There are several datasets for name for male or female classification. In subseuqent iterations, this could be expanded to included more classifications.\n",
        "\n",
        "Below is the source from NLTK, which only has male and female available but could be used for the purposes of this assignment.\n",
        "\n",
        "```\n",
        "names = nltk.corpus.names\n",
        "names.fileids()\n",
        "['female.txt', 'male.txt']\n",
        "male_names = names.words('male.txt')\n",
        "female_names = names.words('female.txt')\n",
        "[w for w in male_names if w in female_names]\n",
        "['Abbey', 'Abbie', 'Abby', 'Addie', 'Adrian', 'Adrien', 'Ajay', 'Alex', 'Alexis',\n",
        "'Alfie', 'Ali', 'Alix', 'Allie', 'Allyn', 'Andie', 'Andrea', 'Andy', 'Angel',\n",
        "'Angie', 'Ariel', 'Ashley', 'Aubrey', 'Augustine', 'Austin', 'Averil', ...]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iy4xZSh_XXDD"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Input\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7_RaovgYME6",
        "outputId": "ba3d5e10-bdbc-4b3a-85b1-b7fab34b5016"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package names to /root/nltk_data...\n",
            "[nltk_data]   Package names is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('names')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKQa86RpY5rH",
        "outputId": "7994a2e1-3827-4ab2-947b-aa9169e4ef67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Abbey', 'Abbie', 'Abby', 'Addie', 'Adrian', 'Adrien', 'Ajay', 'Alex', 'Alexis', 'Alfie', 'Ali', 'Alix', 'Allie', 'Allyn', 'Andie', 'Andrea', 'Andy', 'Angel', 'Angie', 'Ariel', 'Ashley', 'Aubrey', 'Augustine', 'Austin', 'Averil', 'Barrie', 'Barry', 'Beau', 'Bennie', 'Benny', 'Bernie', 'Bert', 'Bertie', 'Bill', 'Billie', 'Billy', 'Blair', 'Blake', 'Bo', 'Bobbie', 'Bobby', 'Brandy', 'Brett', 'Britt', 'Brook', 'Brooke', 'Brooks', 'Bryn', 'Cal', 'Cam', 'Cammy', 'Carey', 'Carlie', 'Carlin', 'Carmine', 'Carroll', 'Cary', 'Caryl', 'Casey', 'Cass', 'Cat', 'Cecil', 'Chad', 'Chris', 'Chrissy', 'Christian', 'Christie', 'Christy', 'Clair', 'Claire', 'Clare', 'Claude', 'Clem', 'Clemmie', 'Cody', 'Connie', 'Constantine', 'Corey', 'Corrie', 'Cory', 'Courtney', 'Cris', 'Daffy', 'Dale', 'Dallas', 'Dana', 'Dani', 'Daniel', 'Dannie', 'Danny', 'Darby', 'Darcy', 'Darryl', 'Daryl', 'Deane', 'Del', 'Dell', 'Demetris', 'Dennie', 'Denny', 'Devin', 'Devon', 'Dion', 'Dionis', 'Dominique', 'Donnie', 'Donny', 'Dorian', 'Dory', 'Drew', 'Eddie', 'Eddy', 'Edie', 'Elisha', 'Emmy', 'Erin', 'Esme', 'Evelyn', 'Felice', 'Fran', 'Francis', 'Frank', 'Frankie', 'Franky', 'Fred', 'Freddie', 'Freddy', 'Gabriel', 'Gabriell', 'Gail', 'Gale', 'Gay', 'Gayle', 'Gene', 'George', 'Georgia', 'Georgie', 'Geri', 'Germaine', 'Gerri', 'Gerry', 'Gill', 'Ginger', 'Glen', 'Glenn', 'Grace', 'Gretchen', 'Gus', 'Haleigh', 'Haley', 'Hannibal', 'Harley', 'Hazel', 'Heath', 'Henrie', 'Hilary', 'Hillary', 'Holly', 'Ike', 'Ikey', 'Ira', 'Isa', 'Isador', 'Isadore', 'Jackie', 'Jaime', 'Jamie', 'Jan', 'Jean', 'Jere', 'Jermaine', 'Jerrie', 'Jerry', 'Jess', 'Jesse', 'Jessie', 'Jo', 'Jodi', 'Jodie', 'Jody', 'Joey', 'Jordan', 'Juanita', 'Jude', 'Judith', 'Judy', 'Julie', 'Justin', 'Karel', 'Kellen', 'Kelley', 'Kelly', 'Kelsey', 'Kerry', 'Kim', 'Kip', 'Kirby', 'Kit', 'Kris', 'Kyle', 'Lane', 'Lanny', 'Lauren', 'Laurie', 'Lee', 'Leigh', 'Leland', 'Lesley', 'Leslie', 'Lin', 'Lind', 'Lindsay', 'Lindsey', 'Lindy', 'Lonnie', 'Loren', 'Lorne', 'Lorrie', 'Lou', 'Luce', 'Lyn', 'Lynn', 'Maddie', 'Maddy', 'Marietta', 'Marion', 'Marlo', 'Martie', 'Marty', 'Mattie', 'Matty', 'Maurise', 'Max', 'Maxie', 'Mead', 'Meade', 'Mel', 'Meredith', 'Merle', 'Merrill', 'Merry', 'Meryl', 'Michal', 'Michel', 'Michele', 'Mickie', 'Micky', 'Millicent', 'Morgan', 'Morlee', 'Muffin', 'Nat', 'Nichole', 'Nickie', 'Nicky', 'Niki', 'Nikki', 'Noel', 'Ollie', 'Page', 'Paige', 'Pat', 'Patrice', 'Patsy', 'Pattie', 'Patty', 'Pen', 'Pennie', 'Penny', 'Perry', 'Phil', 'Pooh', 'Quentin', 'Quinn', 'Randi', 'Randie', 'Randy', 'Ray', 'Regan', 'Reggie', 'Rene', 'Rey', 'Ricki', 'Rickie', 'Ricky', 'Rikki', 'Robbie', 'Robin', 'Ronnie', 'Ronny', 'Rory', 'Ruby', 'Sal', 'Sam', 'Sammy', 'Sandy', 'Sascha', 'Sasha', 'Saundra', 'Sayre', 'Scotty', 'Sean', 'Shaine', 'Shane', 'Shannon', 'Shaun', 'Shawn', 'Shay', 'Shayne', 'Shea', 'Shelby', 'Shell', 'Shelley', 'Sibyl', 'Simone', 'Sonnie', 'Sonny', 'Stacy', 'Sunny', 'Sydney', 'Tabbie', 'Tabby', 'Tallie', 'Tally', 'Tammie', 'Tammy', 'Tate', 'Ted', 'Teddie', 'Teddy', 'Terri', 'Terry', 'Theo', 'Tim', 'Timmie', 'Timmy', 'Tobe', 'Tobie', 'Toby', 'Tommie', 'Tommy', 'Tony', 'Torey', 'Trace', 'Tracey', 'Tracie', 'Tracy', 'Val', 'Vale', 'Valentine', 'Van', 'Vin', 'Vinnie', 'Vinny', 'Virgie', 'Wallie', 'Wallis', 'Wally', 'Whitney', 'Willi', 'Willie', 'Willy', 'Winnie', 'Winny', 'Wynn']\n"
          ]
        }
      ],
      "source": [
        "# Import the NLTK library and access its corpus module, specifically the names corpus\n",
        "names = nltk.corpus.names\n",
        "\n",
        "# Retrieve the file identifiers for the names corpus\n",
        "names.fileids()\n",
        "\n",
        "# Retrieve a list of male names from the names corpus\n",
        "male_names = names.words('male.txt')\n",
        "\n",
        "# Retrieve a list of female names from the names corpus\n",
        "female_names = names.words('female.txt')\n",
        "\n",
        "# Find any names that appear in both the male and female name lists\n",
        "k=[w for w in male_names if w in female_names]\n",
        "\n",
        "# Print the common names found in both the male and female name lists\n",
        "print(k)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtID2pRfZJop",
        "outputId": "76819d67-5e5f-47a7-90ed-b81db5b3ff76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "199/199 [==============================] - 12s 47ms/step - loss: 0.5665 - accuracy: 0.6952 - val_loss: 0.4912 - val_accuracy: 0.7445\n",
            "Epoch 2/10\n",
            "199/199 [==============================] - 3s 13ms/step - loss: 0.3401 - accuracy: 0.8516 - val_loss: 0.5490 - val_accuracy: 0.7565\n",
            "Epoch 3/10\n",
            "199/199 [==============================] - 3s 15ms/step - loss: 0.2088 - accuracy: 0.9078 - val_loss: 0.5448 - val_accuracy: 0.7533\n",
            "Epoch 4/10\n",
            "199/199 [==============================] - 5s 25ms/step - loss: 0.1719 - accuracy: 0.9185 - val_loss: 0.6704 - val_accuracy: 0.7401\n",
            "Epoch 5/10\n",
            "199/199 [==============================] - 4s 18ms/step - loss: 0.1499 - accuracy: 0.9249 - val_loss: 0.7898 - val_accuracy: 0.7275\n",
            "Epoch 6/10\n",
            "199/199 [==============================] - 2s 11ms/step - loss: 0.1323 - accuracy: 0.9300 - val_loss: 0.9030 - val_accuracy: 0.7476\n",
            "Epoch 7/10\n",
            "199/199 [==============================] - 2s 11ms/step - loss: 0.1191 - accuracy: 0.9327 - val_loss: 0.9308 - val_accuracy: 0.7451\n",
            "Epoch 8/10\n",
            "199/199 [==============================] - 2s 12ms/step - loss: 0.1096 - accuracy: 0.9419 - val_loss: 1.0446 - val_accuracy: 0.7508\n",
            "Epoch 9/10\n",
            "199/199 [==============================] - 3s 15ms/step - loss: 0.1060 - accuracy: 0.9393 - val_loss: 0.8888 - val_accuracy: 0.7527\n",
            "Epoch 10/10\n",
            "199/199 [==============================] - 2s 12ms/step - loss: 0.1004 - accuracy: 0.9413 - val_loss: 0.9066 - val_accuracy: 0.7420\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.9066 - accuracy: 0.7420\n",
            "Test Accuracy: 74.20%\n"
          ]
        }
      ],
      "source": [
        "# Combine the lists of male and female names into one list\n",
        "all_names = male_names + female_names\n",
        "\n",
        "# Create labels: 0 for male names and 1 for female names, using the length of male and female names lists\n",
        "labels = [0] * len(male_names) + [1] * len(female_names)\n",
        "\n",
        "# Split the combined names and labels into training and testing sets, with 20% for testing\n",
        "train_names, test_names, train_labels, test_labels = train_test_split(all_names, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the BERT tokenizer from the pre-trained 'bert-base-uncased' model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenize the training and testing names sequences using the BERT tokenizer\n",
        "train_sequences = tokenizer(train_names, truncation=True, padding=True, return_tensors='tf')\n",
        "test_sequences = tokenizer(test_names, truncation=True, padding=True, return_tensors='tf')\n",
        "\n",
        "# Convert the labels to NumPy arrays\n",
        "train_labels = np.array(train_labels)\n",
        "test_labels = np.array(test_labels)\n",
        "\n",
        "# Build the model using a Sequential API\n",
        "model = Sequential()\n",
        "\n",
        "# Add an Embedding layer to represent the input words, with a vocabulary size and embedding dimension of 64\n",
        "model.add(Embedding(input_dim=len(tokenizer.get_vocab()), output_dim=64, input_length=train_sequences.input_ids.shape[1]))\n",
        "\n",
        "# Add an LSTM layer with 64 units to process the embedded sequences\n",
        "model.add(LSTM(64))\n",
        "\n",
        "# Add a Dense layer with one unit and sigmoid activation for binary classification\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model with Adam optimizer, binary crossentropy loss, and accuracy metric\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model on the training sequences with their corresponding labels, using 10 epochs and a batch size of 32\n",
        "model.fit(train_sequences.input_ids, train_labels, epochs=10, batch_size=32, validation_data=(test_sequences.input_ids, test_labels))\n",
        "\n",
        "# Evaluate the trained model on the testing sequences and labels\n",
        "loss, accuracy = model.evaluate(test_sequences.input_ids, test_labels)\n",
        "\n",
        "# Print the test accuracy\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hskj9HbjqSqt",
        "outputId": "5ebd0bd0-cdcb-47b2-aae6-0a32570e50eb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_16 (InputLayer)          [(None, 7)]          0           []                               \n",
            "                                                                                                  \n",
            " input_17 (InputLayer)          [(None, 7)]          0           []                               \n",
            "                                                                                                  \n",
            " input_18 (InputLayer)          [(None, 7)]          0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_for_sequence_classific  TFSequenceClassifie  109483778  ['input_16[0][0]',               \n",
            " ation_5 (TFBertForSequenceClas  rOutput(loss=None,               'input_17[0][0]',               \n",
            " sification)                    logits=(None, 2),                 'input_18[0][0]']               \n",
            "                                 hidden_states=None                                               \n",
            "                                , attentions=None)                                                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,483,778\n",
            "Trainable params: 109,483,778\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/10\n",
            "199/199 [==============================] - 84s 104ms/step - loss: 0.4220 - accuracy: 0.7991\n",
            "Epoch 2/10\n",
            "199/199 [==============================] - 21s 104ms/step - loss: 0.2850 - accuracy: 0.8899\n",
            "Epoch 3/10\n",
            "199/199 [==============================] - 18s 91ms/step - loss: 0.2303 - accuracy: 0.9105\n",
            "Epoch 4/10\n",
            "199/199 [==============================] - 18s 91ms/step - loss: 0.1865 - accuracy: 0.9256\n",
            "Epoch 5/10\n",
            "199/199 [==============================] - 20s 98ms/step - loss: 0.1601 - accuracy: 0.9320\n",
            "Epoch 6/10\n",
            "199/199 [==============================] - 19s 97ms/step - loss: 0.1323 - accuracy: 0.9410\n",
            "Epoch 7/10\n",
            "199/199 [==============================] - 19s 94ms/step - loss: 0.1217 - accuracy: 0.9463\n",
            "Epoch 8/10\n",
            "199/199 [==============================] - 19s 95ms/step - loss: 0.1040 - accuracy: 0.9525\n",
            "Epoch 9/10\n",
            "199/199 [==============================] - 18s 91ms/step - loss: 0.0989 - accuracy: 0.9507\n",
            "Epoch 10/10\n",
            "199/199 [==============================] - 19s 94ms/step - loss: 0.0888 - accuracy: 0.9545\n",
            "50/50 [==============================] - 5s 40ms/step - loss: 0.6179 - accuracy: 0.8559\n",
            "Test Accuracy: 0.8559\n"
          ]
        }
      ],
      "source": [
        "# Convert the training and testing sequences to NumPy arrays\n",
        "train_input_ids = train_sequences['input_ids'].numpy()\n",
        "train_attention_mask = train_sequences['attention_mask'].numpy()\n",
        "train_token_type_ids = train_sequences['token_type_ids'].numpy()\n",
        "\n",
        "test_input_ids = test_sequences['input_ids'].numpy()\n",
        "test_attention_mask = test_sequences['attention_mask'].numpy()\n",
        "test_token_type_ids = test_sequences['token_type_ids'].numpy()\n",
        "\n",
        "# Convert the labels to NumPy arrays\n",
        "train_labels = np.array(train_labels)\n",
        "test_labels = np.array(test_labels)\n",
        "\n",
        "# Define input layers for input_ids, attention_mask, and token_type_ids\n",
        "input_ids = Input(shape=(train_input_ids.shape[1],), dtype=tf.int32)\n",
        "attention_mask = Input(shape=(train_attention_mask.shape[1],), dtype=tf.int32)\n",
        "token_type_ids = Input(shape=(train_token_type_ids.shape[1],), dtype=tf.int32)\n",
        "\n",
        "# Load pre-trained BERT model for sequence classification\n",
        "bert_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Connect the input layers to the BERT model\n",
        "outputs = bert_model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "\n",
        "# Build the model using the BERT model outputs\n",
        "model = Model(inputs=[input_ids, attention_mask, token_type_ids], outputs=outputs.logits)\n",
        "\n",
        "# Compile the model with Adam optimizer, sparse categorical crossentropy loss, and accuracy metric\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()\n",
        "\n",
        "# Train the model on the training data with batch size of 32 and 10 epochs\n",
        "model.fit([train_input_ids, train_attention_mask, train_token_type_ids], train_labels, batch_size=32, epochs=10)\n",
        "\n",
        "# Evaluate the trained model on the testing data\n",
        "loss, accuracy = model.evaluate([test_input_ids, test_attention_mask, test_token_type_ids], test_labels)\n",
        "\n",
        "# Print the test accuracy\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExMITGgCdQWz"
      },
      "source": [
        "# References\n",
        "1. https://arxiv.org/pdf/2102.03692.pdf\n",
        "2. https://alvinntnu.github.io/NTNU_ENC2045_LECTURES/exercise/13-attention.html\n",
        "3. https://towardsdatascience.com/deep-learning-gender-from-name-lstm-recurrent-neural-networks-448d64553044\n",
        "4. https://www.nltk.org/book/ch02.html#sec-lexical-resources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
